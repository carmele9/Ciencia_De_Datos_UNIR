{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Proyecto de desarrollo de sistema de IA basado en alquileres de pisos – Segunda fase\n",
    "# Carmen De Los Ángeles Camacho Tejada - 02/03/2025\n",
    "# Ciencia De Datos - UNIR\n",
    "# Profesor Alan Sastre - Módulo 2\n",
    "\n",
    "# Importamos las librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*_CARGA Y EXPLORACIÓN INICIAL DEL DATASET_*",
   "id": "3da53f14a2777e7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"madrid_dataset_idealista.csv\")\n",
    "df.head()"
   ],
   "id": "c0090c1661b5440c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "da427100662122b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe()",
   "id": "a522377d7b713e19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para la siguiente parte de la exploración del dataset, dejo un pequeño esquema con los pasos que voy a seguir a continuación:\n",
    "- Elimina duplicados con drop_duplicates().\n",
    "- Revisa valores nulos con isnull().sum().\n",
    "- Muestra los tipos de datos actuales con dtypes.\n",
    "- Convierte fechas (last update) a formato datetime, evitando errores con errors='coerce'.\n",
    "- Convierte variables categóricas (district, subdistrict, orientation) a category.\n",
    "- Convierte variables binarias (garage_included, furnished, balcony) a int.\n",
    "- Muestra estadísticas de price, floor_area y year_built para detectar valores extraños.\n",
    "- Identifica valores fuera de rango, por ejemplo, negativos en price o floor_area.\n"
   ],
   "id": "2888bfecce2fa8a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar duplicados\n",
    "df = df.drop_duplicates()"
   ],
   "id": "49d090dc206c3d18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Revisar valores nulos\n",
    "df.isnull().sum()"
   ],
   "id": "1cefc0586c5231e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificar inconsistencias en los tipos de datos\n",
    "df.dtypes"
   ],
   "id": "dc1de6263b2e9d1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convertir la columna de fecha a formato datetime\n",
    "if 'last update' in df.columns:\n",
    "    df['last update'] = pd.to_datetime(df['last update'], errors='coerce')"
   ],
   "id": "e6545167eb15dbe6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convertir variables categóricas a tipo category\n",
    "categorical_columns = ['district', 'subdistrict', 'orientation']\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')"
   ],
   "id": "12b8273742142dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convertir variables binarias a 0/1\n",
    "binary_columns = ['garage_included', 'furnished', 'balcony']\n",
    "for col in binary_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(int)"
   ],
   "id": "6d52979c84ea093c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verificar valores extraños en price, floor_area y year_built\n",
    "df[['price', 'floor_area', 'year_built']].describe()"
   ],
   "id": "f92f7fee0b2f7641",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verificar si hay valores negativos o fuera de un rango lógico\n",
    "for col in ['price', 'floor_area', 'year_built']:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nValores fuera de rango en {col}:\")\n",
    "        print(df[df[col] < 0])"
   ],
   "id": "597be8a755ad247d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*_Limpieza y validación de datos_*\n",
   "id": "450828979f4d12d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para el siguiente paso, dejo a continuación un pequeño esquema de aquello que voy a realizar:\n",
    "\n",
    "- Manejo de valores nulos\n",
    "- Variables categóricas: Se rellenan con la moda\n",
    "- Variables numéricas: Se prueban SimpleImputer (mediana), KNNImputer e IterativeImputer.\n",
    "- Se comparan las estadísticas de cada método para elegir el mejor.\n",
    "- Detección y tratamiento de outliers\n",
    "- Boxplots para visualizar valores extremos.\n",
    "- IQR para detectar outliers en price, floor_area, bedrooms.\n",
    "- Winsorization en lugar de eliminar datos para evitar pérdida excesiva.\n",
    "- Transformación de variables con sesgo alto\n",
    "- Se usa log-transform (np.log1p()).\n",
    "- Se usa Box-Cox / Yeo-Johnson para mejorar normalidad.\n",
    "- Se comparan histogramas antes y después de la transformación.\n",
    "- Se revisa el sesgo (skew()) para verificar la mejora."
   ],
   "id": "764456ed4080e374"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PowerTransformer"
   ],
   "id": "5e21484375f8d938",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### 1. Manejo de valores nulos\n",
    "\n",
    "# Variables categóricas: rellenar con la moda\n",
    "cat_cols = ['district', 'subdistrict', 'orientation']\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        mode_value = df[col].mode()[0]  # Moda\n",
    "        df.fillna({col: mode_value}, inplace=True)  # Solución recomendada"
   ],
   "id": "c0c395cfe158a8b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Variables numéricas: aplicar diferentes imputers\n",
    "num_cols = ['price', 'floor_area', 'year_built', 'bedrooms', 'bathrooms']\n",
    "\n",
    "# SimpleImputer con la mediana\n",
    "simple_imputer = SimpleImputer(strategy='median')\n",
    "df_simple = df.copy()\n",
    "df_simple[num_cols] = simple_imputer.fit_transform(df_simple[num_cols])\n",
    "\n",
    "# KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn = df.copy()\n",
    "df_knn[num_cols] = knn_imputer.fit_transform(df_knn[num_cols])\n",
    "\n",
    "# IterativeImputer\n",
    "iter_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "df_iter = df.copy()\n",
    "df_iter[num_cols] = iter_imputer.fit_transform(df_iter[num_cols])\n",
    "\n",
    "# Evaluar cuál imputación es mejor (comparando estadísticas)\n",
    "print(\"SimpleImputer Stats:\\n\", df_simple[num_cols].describe())\n",
    "print(\"\\nKNNImputer Stats:\\n\", df_knn[num_cols].describe())\n",
    "print(\"\\nIterativeImputer Stats:\\n\", df_iter[num_cols].describe())"
   ],
   "id": "e6ecf7417d48a8d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Comparación de Imputers:\n",
    "\n",
    "1. SimpleImputer (Mediana o Moda)\n",
    "Ventajas: Método sencillo y rápido. No altera significativamente la distribución original de los datos. Útil si hay pocos valores nulos.\n",
    "Desventajas: Puede no capturar relaciones entre variables. Mantiene valores repetidos (ejemplo: floor_area = 75 en 25%-75%).\n",
    "Aquí, floor_area tiene una alta concentración en 75, lo que sugiere que la mediana no representa bien la distribución real. No introduce valores fuera del rango original.\n",
    "2. KNNImputer\n",
    "Ventajas: Considera valores cercanos en otras variables para imputar datos. Puede capturar patrones en los datos mejor que el SimpleImputer.\n",
    "Desventajas: Más costoso computacionalmente. Puede distorsionar datos si la cantidad de nulos es alta.\n",
    "Aquí, floor_area tiene una media más alta (100.42 vs. 83.26 en SimpleImputer), lo que sugiere que KNN está elevando los valores imputados. year_built muestra una mayor dispersión, lo cual puede ser bueno o malo dependiendo de la estructura real de los datos.\n",
    "3. IterativeImputer\n",
    "Ventajas: Usa regresión para estimar valores, lo que permite una imputación más precisa. Aprende de todas las variables disponibles.\n",
    "Desventajas: Computacionalmente más costoso. Puede generar valores fuera del rango original.\n",
    "Aquí, floor_area tiene valores negativos (-29.92), lo cual es inválido. year_built tiene valores fuera del rango normal (2109), lo que sugiere que está extrapolando más de lo debido.\n",
    "\n",
    "Para esta actividad, no se especifica nada sobre el costo computacional, por ende, se usará KNNImputer, ya que, captura patrones complejos y tienes pocos valores nulos."
   ],
   "id": "b5c1c083360158bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df[num_cols] = knn_imputer.fit_transform(df[num_cols])"
   ],
   "id": "8f126bf2de5098a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Para los gráficos, se busca aplicar un estilo visual profesional\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"figure.figsize\": (12, 6)\n",
    "})\n",
    "\n",
    "### 2. Detección y Tratamiento de Outliers\n",
    "\n",
    "# Visualización con Boxplots antes del tratamiento\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[['price', 'floor_area', 'bedrooms']], palette=\"viridis\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplot de Variables Numéricas Antes de Tratar Outliers\", fontsize=14)\n",
    "plt.show()\n"
   ],
   "id": "5dc208ac773f1a31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Función para tratar outliers con IQR (Winsorization)\n",
    "def treat_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    return df\n"
   ],
   "id": "44f62ce4664fb5e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Aplicar Winsorization a price, floor_area y bedrooms\n",
    "for col in ['price', 'floor_area', 'bedrooms']:\n",
    "    df = treat_outliers_iqr(df, col)"
   ],
   "id": "b639dca4977d6981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualización con Boxplots después del tratamiento\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[['price', 'floor_area', 'bedrooms']], palette=\"deep\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplot de Variables Numéricas Después de Tratar Outliers\", fontsize=14)\n",
    "plt.show()"
   ],
   "id": "e61ad5b181d18657",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### 3. Transformación de Datos con Sesgo Alto\n",
    "\n",
    "# Verificar sesgo antes de la transformación\n",
    "print(\"\\n Skewness antes de la transformación:\")\n",
    "print(df[['price', 'floor_area']].skew())\n",
    "\n",
    "# Aplicar Transformaciones\n",
    "df['price_log'] = np.log1p(df['price'])\n",
    "df['floor_area_log'] = np.log1p(df['floor_area'])\n",
    "\n",
    "# Aplicar PowerTransformer (Box-Cox/Yeo-Johnson)\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "df[['price_bc', 'floor_area_bc']] = pt.fit_transform(df[['price', 'floor_area']])\n"
   ],
   "id": "419b9cb2843b6caa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Comparación de Histogramas antes y después de la transformación\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), constrained_layout=True)\n",
    "colors = [\"#7babb3\", \"#ff7f98\", \"#8ac767\"]  # Azul, Rojo, Verde\n",
    "\n",
    "sns.histplot(df['price'], bins=50, kde=True, color=colors[0], ax=axes[0])\n",
    "axes[0].set_title(\" Distribución Original de Price\", fontsize=12)\n",
    "axes[0].axvline(df['price'].mean(), color='black', linestyle='dashed', linewidth=1)\n",
    "\n",
    "sns.histplot(df['price_log'], bins=50, kde=True, color=colors[1], ax=axes[1])\n",
    "axes[1].set_title(\" Distribución con Log Transform\", fontsize=12)\n",
    "axes[1].axvline(df['price_log'].mean(), color='black', linestyle='dashed', linewidth=1)\n",
    "\n",
    "sns.histplot(df['price_bc'], bins=50, kde=True, color=colors[2], ax=axes[2])\n",
    "axes[2].set_title(\" Distribución con Box-Cox/Yeo-Johnson\", fontsize=12)\n",
    "axes[2].axvline(df['price_bc'].mean(), color='black', linestyle='dashed', linewidth=1)\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "6f56e6ba78e1ac4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verificar sesgo después de la transformación\n",
    "print(\"\\n Skewness después de la transformación:\")\n",
    "print(df[['price_log', 'floor_area_log', 'price_bc', 'floor_area_bc']].skew())"
   ],
   "id": "f6262e65fd64b0a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*_Análisis exploratorio de datos (EDA)_*\n",
    "\n",
    "Para el siguiente paso, dejo un esquema de lo que voy a hacer a continuación:\n",
    "\n",
    "- Análisis univariante:\n",
    "    - Histogramas para variables numéricas.\n",
    "    - Diagramas de densidad para estudiar distribuciones.\n",
    "    - Estadísticas descriptivas\n",
    "- Análisis bivariante:\n",
    "    - Correlación entre variables numéricas.\n",
    "    - Gráficos de dispersión para ver relación entre price y floor_area.\n",
    "    - Gráficos categóricos para ver distribución de price según district.\n",
    "- Análisis multivariante:\n",
    "    - Gráficos de pares para identificar relaciones entre múltiples variables.\n",
    "    - Mapas de calor de correlación para detectar multicolinealidad.\n",
    "    - Aplicar PCA para explorar la dimensionalidad de los datos.\n"
   ],
   "id": "73c50a4a101d3dcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Histograma para 'price'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df['price'], kde=True, color=\"skyblue\", bins=30, edgecolor=\"black\")\n",
    "plt.title('Distribución de Precio', fontsize=16)\n",
    "plt.xlabel('Precio (€)', fontsize=12)\n",
    "plt.ylabel('Frecuencia', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Histograma para 'floor_area'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df['floor_area'], kde=True, color=\"coral\", bins=30, edgecolor=\"black\")\n",
    "plt.title('Distribución de Área de Piso', fontsize=16)\n",
    "plt.xlabel('Área del Piso (m²)', fontsize=12)\n",
    "plt.ylabel('Frecuencia', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "1759f833a91bc6bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Diagrama de Densidad para 'price'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.kdeplot(df['price'], fill=True, color=\"purple\", lw=2)\n",
    "plt.title('Distribución de Densidad del Precio', fontsize=16)\n",
    "plt.xlabel('Precio (€)', fontsize=12)\n",
    "plt.ylabel('Densidad', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Diagrama de Densidad para 'floor_area'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.kdeplot(df['floor_area'], fill=True, color=\"chocolate\", lw=2)\n",
    "plt.title('Distribución de Densidad del Área del Piso', fontsize=16)\n",
    "plt.xlabel('Área del Piso (m²)', fontsize=12)\n",
    "plt.ylabel('Densidad', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "d673157ac17b5ecd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Estadísticas Descriptivas\n",
    "df.describe(percentiles=[0.25, 0.5, 0.75])"
   ],
   "id": "97eb463163163fe1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar las columnas no numéricas\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "corr_matrix = df_numeric.corr()\n",
    "\n",
    "# Mostrar el mapa de calor de la correlación\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title('Mapa de Calor de Correlación', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "id": "9c3add48ae09f1e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gráfico de Dispersión entre 'price' y 'floor_area'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='floor_area', y='price', data=df, color='teal', alpha=0.7)\n",
    "plt.title('Relación entre Área del Piso y Precio', fontsize=16)\n",
    "plt.xlabel('Área del Piso (m²)', fontsize=12)\n",
    "plt.ylabel('Precio (€)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "f1efaeff245a644f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gráfico de Caja para 'price' según 'district'\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(x='district', y='price', data=df, hue='district', palette='Set2', showfliers=False)\n",
    "plt.title('Distribución del Precio por Distrito', fontsize=16)\n",
    "plt.xlabel('Distrito', fontsize=12)\n",
    "plt.ylabel('Precio (€)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "7bd0a70d920cc2de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gráfico de Violín para 'price' según 'district'\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.violinplot(x='district', y='price', data=df, hue='district', split=True, palette='muted', inner=\"quart\")\n",
    "plt.title('Distribución del Precio por Distrito', fontsize=16)\n",
    "plt.xlabel('Distrito', fontsize=12)\n",
    "plt.ylabel('Precio (€)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "5e30ea9c0b2b40f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gráfico de Dispersión entre 'price' y 'year_built'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='year_built', y='price', data=df, color='indianred', alpha=0.7)\n",
    "plt.title('Relación entre Año de Construcción y Precio', fontsize=16)\n",
    "plt.xlabel('Año de Construcción', fontsize=12)\n",
    "plt.ylabel('Precio (€)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "758e15e77d2b1bdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gráfico de Regresión para 'price' y 'floor_area'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x='floor_area', y='price', data=df, scatter_kws={'s': 30}, line_kws={'color': 'red'})\n",
    "plt.title('Regresión entre Área del Piso y Precio', fontsize=16)\n",
    "plt.xlabel('Área del Piso (m²)', fontsize=12)\n",
    "plt.ylabel('Precio (€)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "58d6346d79d68631",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gráfico de barras para 'price' según 'terrace'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='terrace', y='price', data=df, palette='pastel', hue= 'terrace', legend=False)\n",
    "plt.title('Precio según la Presencia de Terraza', fontsize=16)\n",
    "plt.xlabel('Tiene Terraza', fontsize=12)\n",
    "plt.ylabel('Precio (€)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "2a44b3e84e816a24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(df, x='floor_area', y='price', title='Relación entre Área y Precio',\n",
    "                 labels={'floor_area': 'Área (m²)', 'price': 'Precio'},\n",
    "                 color='district', # Si quieres diferenciar por distrito\n",
    "                 template='plotly_dark')  # O 'ggplot2', 'seaborn', etc.\n",
    "fig.show()\n"
   ],
   "id": "265ff21548ca6431",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.box(df, x='district', y='price', title='Distribución de Precio según Distrito',\n",
    "             labels={'district': 'Distrito', 'price': 'Precio'},\n",
    "             template='plotly_dark')\n",
    "fig.show()\n"
   ],
   "id": "1cf2225757c3ef15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "corr_matrix = df_numeric.corr()\n",
    "fig = go.Figure(data=go.Heatmap(z=corr_matrix.values,\n",
    "                               x=corr_matrix.columns,\n",
    "                               y=corr_matrix.columns,\n",
    "                               colorscale='Viridis'))\n",
    "fig.update_layout(title='Mapa de Calor de Correlación')\n",
    "fig.show()\n"
   ],
   "id": "537ab278494a7543",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.scatter_matrix(df, dimensions=['floor_area', 'price', 'bedrooms', 'bathrooms'],\n",
    "                        title='Gráfico de Pares', color='district')\n",
    "fig.show()\n"
   ],
   "id": "cba956e17a08582d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.scatter_3d(df, x='floor_area', y='price', z='bedrooms', color='district',\n",
    "                    title='Gráfico de Dispersión 3D: Área, Precio y Habitaciones',\n",
    "                    labels={'floor_area': 'Área (m²)', 'price': 'Precio', 'bedrooms': 'Habitaciones'})\n",
    "fig.show()\n"
   ],
   "id": "15f5b668835b10df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Seleccionamos las columnas numéricas\n",
    "df_numeric = df[['floor_area', 'price', 'bedrooms', 'bathrooms']]\n",
    "\n",
    "# Normalizamos las variables\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_numeric)\n",
    "\n",
    "# Realizamos PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Creamos un DataFrame con las dos primeras componentes principales\n",
    "pca_df = pd.DataFrame(data=pca_components, columns=['PCA1', 'PCA2'])\n",
    "\n",
    "# Agregamos el 'district' para la visualización\n",
    "pca_df['district'] = df['district']\n",
    "\n",
    "# Graficamos los componentes principales\n",
    "fig = px.scatter(pca_df, x='PCA1', y='PCA2', color='district', title='PCA: Componentes Principales',\n",
    "                 labels={'PCA1': 'Componente Principal 1', 'PCA2': 'Componente Principal 2'})\n",
    "fig.show()\n"
   ],
   "id": "713686c3ab3193e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar la varianza explicada por cada componente\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Varianza explicada por PCA1: {explained_variance[0]:.2f}\")\n",
    "print(f\"Varianza explicada por PCA2: {explained_variance[1]:.2f}\")\n"
   ],
   "id": "787d39f50fcdca1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*_Preparación de los datos para el modelado_*\n",
    "\n",
    "Aquí le adjunto un listado de los pasos que voy a seguir:\n",
    "\n",
    "- Codificación de Variables Categóricas: One-Hot Encoding para variables con múltiples categorías.\n",
    "- Escalado de Variables Numéricas: Probamos StandardScaler.\n",
    "- Segmentación en Entrenamiento y Validación: Usamos train_test_split para dividir los datos.\n",
    "- Pipeline de Preprocesamiento: Combinamos todos los pasos anteriores en un Pipeline para facilitar el flujo de trabajo.\n"
   ],
   "id": "8a8758d1f8e318e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lista de columnas que no son necesarias para las predicciones:\n",
    "# web_id: Aunque es único, no es útil para la predicción del precio.\n",
    "# url: Solo es un identificador de la propiedad, no aporta información útil para el modelo.\n",
    "# title: Puede ser útil en un análisis de texto, pero en general, no es necesario para este ejercicio.\n",
    "# professional_name: Es un dato sobre la agencia, pero puede no tener un gran impacto directo en el precio.\n",
    "# last update: Es una variable temporal que, en la mayoría de los casos, no tiene relevancia para el precio del alquiler.\n",
    "columns_to_drop = [\n",
    "    'web_id', 'url', 'title', 'professional_name', 'last_update'\n",
    "]\n",
    "# Eliminar las columnas\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)"
   ],
   "id": "a0ff932b6ff91133",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Seleccionamos las columnas numéricas para escalar\n",
    "categorical_cols = ['type', 'floor', 'orientation', 'district', 'subdistrict', 'location']\n",
    "\n",
    "# Usar LabelEncoder para convertir las columnas categóricas a valores numéricos\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "binary_cols = [\n",
    "    'private_owner', 'second_hand', 'lift', 'equipped_kitchen', 'fitted_wardrobes', 'air_conditioning',\n",
    "    'terrace', 'storeroom', 'swimming_pool', 'garden_area'\n",
    "]\n",
    "for col in binary_cols:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "numerical_cols = [\n",
    "    'deposit', 'floor_built', 'floor_area', 'year_built', 'bedrooms', 'bathrooms',\n",
    "    'garage_included', 'furnished', 'balcony', 'postalcode', 'floor_area_log', 'floor_area_bc'\n",
    "]\n"
   ],
   "id": "132e027231f4d120",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pipeline de preprocesamiento para las variables categóricas\n",
    "# Obtener todas las categorías posibles combinando X_train y X_val\n",
    "categorias_completas = [np.unique(df[col]) for col in categorical_cols]\n",
    "\n",
    "# OneHotEncoder con categorías predefinidas\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(categories=categorias_completas, drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Pipeline de preprocesamiento para variables binarias\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "])\n",
    "\n",
    "# Pipeline de preprocesamiento para las variables numéricas\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ],
   "id": "538c047f40f2bf33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Crear el preprocesador final\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        ('bin', binary_transformer, binary_cols)\n",
    "    ], remainder='passthrough'\n",
    ")"
   ],
   "id": "5d38e03a16c28692",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Primero, se hará el Modelado con Regresión para predecir el precio.",
   "id": "47e530d3fe4485f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dividir el conjunto de datos en entrenamiento y validación\n",
    "X = df.drop(columns=['price'])  # Variables independientes (sin la columna 'price')\n",
    "y = df['price']  # Variable dependiente (target)\n",
    "\n",
    "# Dividir en entrenamiento y validación (80% / 20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "3cfc41525dc54023",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*_Modelado_*\n",
    "- _Regresión (Predicción del Precio)_\n",
    "+ Modelos a probar:\n",
    "- Regresión Lineal\n",
    "- Árboles de Decisión\n",
    "- Random Forest\n",
    "- GradientBoostingRegressor\n",
    "- SVR\n",
    "- KNeighborsRegressor\n",
    "- Red Neuronal Pequeña con TensorFlow\n",
    "\n",
    "+ Métricas de evaluación:\n",
    "- MSE (Error Cuadrático Medio)\n",
    "- RMSE (Raíz del MSE)\n",
    "- MAPE (Error Absoluto Porcentual Medio)\n",
    "- Validación Cruzada\n",
    "\n"
   ],
   "id": "f1aaaebf7c6a73f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Explicación de los modelos a usar y de las métricas con las que se evaluará:\n",
    "- Regresión Lineal: Es un modelo simple, ideal como referencia.\n",
    "- Árboles de Decisión: Útil para capturar no linealidades y relaciones complejas en los datos.\n",
    "- Random Forest: Un modelo de ensamblaje que mejora la robustez y la precisión de los árboles de decisión.\n",
    "- SVR (Support Vector Regression): Bueno para capturar relaciones no lineales.\n",
    "- K-Neighbors Regressor: Utiliza la proximidad entre las muestras para predecir los valores.\n",
    "- Gradient Boosting: Un modelo de ensamblaje basado en árboles que es muy efectivo en problemas de regresión.\n",
    "- Red Neuronal: Implementada en TensorFlow, modela relaciones complejas y no lineales en los datos.\n",
    "- MSE (Mean Squared Error): Penaliza fuertemente los errores grandes.\n",
    "- RMSE (Root Mean Squared Error): Raíz cuadrada del MSE, más fácil de interpretar en las mismas unidades que el objetivo (precio).\n",
    "- MAPE (Mean Absolute Percentage Error): Mide el error porcentual medio entre las predicciones y los valores reales."
   ],
   "id": "9c493b4adffb0aab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from keras.src.layers import Dense\n",
    "from keras import Input, Sequential\n"
   ],
   "id": "e12a29daae857ddc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Crear un pipeline de modelado con el preprocesador y el modelo\n",
    "def create_model(model):\n",
    "    return Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n"
   ],
   "id": "e68436992f360425",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Modelos a probar\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'SVR': SVR(),\n",
    "    'K-Neighbors': KNeighborsRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'Neural Network': None  # Se verá por separado, al ser con TensorFlow\n",
    "}\n",
    "\n",
    "# Diccionario de hiperparámetros solo para los modelos que admiten GridSearch\n",
    "param_grids = {\n",
    "    'Decision Tree': {\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "        'model__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model__n_estimators': [50, 100],\n",
    "        'model__max_depth': [5, 10, 20],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "        'model__min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model__kernel': ['linear', 'rbf'],\n",
    "        'model__C': [0.1, 1, 10]\n",
    "    },\n",
    "    'K-Neighbors': {\n",
    "        'model__n_neighbors': [3, 5, 10],\n",
    "        'model__weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 10]\n",
    "    }\n",
    "}"
   ],
   "id": "42a63dd22985c2f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Esta celda dura casi 10 minutos al hacer muchas cosas de golpe\n",
    "# Resultados para almacenar métricas de cada modelo\n",
    "results = {}\n",
    "\n",
    "# Entrenamiento y evaluación de los modelos de Scikit-Learn\n",
    "for model_name, model in models.items():\n",
    "    if model_name != 'Neural Network':  # Modelos de Scikit-Learn\n",
    "        pipeline = create_model(model)\n",
    "\n",
    "        if model_name in param_grids:  # Si el modelo tiene hiperparámetros ajustables\n",
    "            print(f\"\\nEjecutando GridSearch para {model_name}...\")\n",
    "            grid_search = GridSearchCV(pipeline, param_grids[model_name],\n",
    "                                       cv=5, scoring='neg_mean_squared_error',\n",
    "                                       n_jobs=-1, verbose=2)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # Guardar el mejor modelo encontrado\n",
    "            best_model = grid_search.best_estimator_\n",
    "            print(f\"Mejores parámetros para {model_name}: {grid_search.best_params_}\")\n",
    "        else:\n",
    "            best_model = pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predicciones\n",
    "        y_pred = best_model.predict(X_val)\n",
    "\n",
    "        # Métricas\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        rmse = root_mean_squared_error(y_val, y_pred)  # Calculamos RMSE con la nueva función\n",
    "        mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "\n",
    "        # Guardamos los resultados\n",
    "        results[model_name] = {'MSE': mse, 'RMSE': rmse, 'MAPE': mape}\n",
    "    else:  # Para la red neuronal con TensorFlow\n",
    "        # Preprocesar los datos\n",
    "        X_train_nn = preprocessor.fit_transform(X_train)\n",
    "        X_val_nn = preprocessor.transform(X_val)\n",
    "\n",
    "        # Crear una red neuronal sencilla\n",
    "        model_nn = Sequential([\n",
    "            Input(shape=(X_train_nn.shape[1],)),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)  # Una única salida (el precio)\n",
    "])\n",
    "\n",
    "        model_nn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "        # Entrenamiento de la red neuronal\n",
    "        model_nn.fit(X_train_nn, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "        # Predicciones\n",
    "        y_pred_nn = model_nn.predict(X_val_nn)\n",
    "\n",
    "        # Métricas\n",
    "        mse_nn = mean_squared_error(y_val, y_pred_nn)\n",
    "        rmse_nn = root_mean_squared_error(y_val, y_pred_nn)  # Calculamos RMSE con la nueva función\n",
    "        mape_nn = mean_absolute_percentage_error(y_val, y_pred_nn)\n",
    "\n",
    "        # Guardamos los resultados\n",
    "        results['Neural Network'] = {'MSE': mse_nn, 'RMSE': rmse_nn, 'MAPE': mape_nn}\n",
    "\n"
   ],
   "id": "a2dc3fe4e6cbf877",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluación de modelos con validación cruzada\n",
    "for model_name, model in models.items():\n",
    "    if model_name != 'Neural Network':  # Los modelos de sklearn\n",
    "        pipeline = create_model(model)\n",
    "        cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "        mean_cv_score = -cv_scores.mean()  # Convertimos de negativo a positivo\n",
    "        print(f\"{model_name} - MSE (Cross-Validation): {mean_cv_score}\")\n"
   ],
   "id": "a3b229ed70bd2986",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar los resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ],
   "id": "9fc70e772c6aa878",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Según la validación cruzada:\n",
    "1. El MSE de la regresión lineal es relativamente alto, lo que indica que el modelo no está ajustando bien los datos. Puede que no esté capturando bien las relaciones no lineales presentes en el conjunto de datos.\n",
    "2. El modelo de Árbol de Decisión tiene un MSE bajo, lo que sugiere que es capaz de ajustar bien el modelo a los datos. Sin embargo, los árboles de decisión son propensos al sobreajuste, por lo que se debe verificar su rendimiento en datos no vistos y considerar regularización si es necesario.\n",
    "3. El modelo de Random Forest tiene el MSE más bajo de todos, lo que indica que está haciendo un excelente trabajo ajustando el modelo a los datos. Random Forest generalmente ofrece un buen balance entre bajo error y resistencia al sobreajuste.\n",
    "4. SVR (Support Vector Regression): El modelo SVR tiene un MSE extremadamente alto, lo que indica que está ajustando mal los datos. Esto podría ser debido a un mal ajuste de los hiperparámetros, como el kernel, la regularización, etc. Es probable que se necesite una mayor sintonización o que no sea adecuado para este problema específico.\n",
    "5. El modelo de K-Neighbors tiene un MSE relativamente alto. Aunque K-Neighbors puede ser útil en problemas donde las relaciones entre las características son locales, su rendimiento puede verse afectado por la elección del número de vecinos.\n",
    "6. El Gradient Boosting muestra un MSE moderado, lo que indica que está ajustando bien los datos, pero no tanto como Random Forest. Gradient Boosting es un modelo muy potente, pero puede ser sensible a la sobreajuste si no se ajustan correctamente los hiperparámetros.\n",
    "\n",
    "\n",
    "En conclusión, Random Forest es el modelo más prometedor."
   ],
   "id": "3cbefc6c615ce5d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aquí está la evaluación de los resultados basados en las métricas MSE, RMSE y MAPE para los modelos:\n",
    "\n",
    "1. La regresión lineal tiene un MSE y RMSE relativamente altos en comparación con otros modelos. Sin embargo, su MAPE es bajo, lo que indica que, a pesar de que el error cuadrático es mayor, el porcentaje de error relativo sobre los valores reales no es tan grande.\n",
    "2. El árbol de decisión tiene un desempeño excelente con valores extremadamente bajos para MSE, RMSE y MAPE, lo que sugiere que está ajustando muy bien a los datos. Sin embargo, es importante revisar si este modelo está sobre ajustando los datos, ya que los árboles de decisión pueden ser propensos a ello.\n",
    "3. Random Forest tiene el mejor desempeño en todas las métricas. El MSE y RMSE son los más bajos, lo que indica un excelente ajuste, y el MAPE también es extremadamente bajo, lo que refleja que los errores relativos son mínimos. Este modelo es el más robusto y preciso en este conjunto de resultados.\n",
    "4. El modelo SVR tiene un MSE y RMSE muy altos, lo que indica un pobre desempeño. Además, el MAPE es relativamente alto, lo que sugiere que los errores del modelo son grandes en relación con los valores reales. Esto sugiere que el modelo no está ajustando bien los datos, y podría ser necesario ajustar los parámetros (como el kernel, la regularización, etc.) o probar otro enfoque.\n",
    "5. K-Neighbors también tiene un MSE y RMSE altos, lo que indica que no está ajustando bien a los datos. Además, el MAPE es alto, lo que muestra que hay un gran error relativo. Al igual que SVR, podría necesitar ajustes en los hiperparámetros.\n",
    "6. Gradient Boosting tiene un rendimiento bastante bueno con un MSE y RMSE bajos. Aunque no es tan bueno como Random Forest, sus valores son aún excelentes, y su MAPE también es bajo, lo que sugiere que el modelo está funcionando bien en términos de error relativo.\n",
    "7. La red neuronal tiene un MSE y RMSE más altos que Random Forest y Gradient Boosting, pero su MAPE es aceptable. Es probable que la red neuronal esté haciendo un buen trabajo ajustando los datos, pero necesitaría ser afinada más en términos de arquitectura y parámetros de entrenamiento.\n",
    "\n",
    "\n",
    "Viendo todo lo dicho, Random Forest es el modelo más recomendable para este problema, dado su desempeño general superior.\n",
    "En el caso de querer experimentar con otros modelos, Decision Tree y Gradient Boosting también tienen buenos resultados y podrían ser opciones viables.\n",
    "SVR y K-Neighbors deben ser ajustados o descartados, ya que no están funcionando bien con este conjunto de datos."
   ],
   "id": "426762cc5aa4183e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3c4238a5ff724d8c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
