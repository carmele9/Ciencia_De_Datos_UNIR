{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Actividad 4: Clasificación multiclase con Scikit-learn y TensorFlow sobre dataset diamonds\n",
    "# Carmen De Los Ángeles Camacho Tejada - 16/02/2025\n",
    "# Ciencia De Datos - UNIR\n",
    "# Profesor Alan Sastre - Módulo 2\n",
    "\n",
    "# Importamos las librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargamos el dataset 'tips' desde seaborn\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/diamonds.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Mostramos las primeras filas del dataset\n",
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*_ANÁLISIS DE ESTADÍSTICAS DESCRIPTIVAS_*",
   "id": "fd8b4409cbf0f647"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Información general del dataset\n",
    "data.info()"
   ],
   "id": "68ce475e92be61da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Estadísticas de las variables numéricas del dataset\n",
    "data.describe()"
   ],
   "id": "d21b6ad390b9f81a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Descripción de las variables categóricas del dataset\n",
    "data.describe(include=['object'])"
   ],
   "id": "b13ebd2a5bc5f5b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verificación de valores nulos\n",
    "data.isnull().sum()"
   ],
   "id": "57369792a2eaa5e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribución de la variable a predecir: 'cut'\n",
    "data['cut'].value_counts()"
   ],
   "id": "32d4414ebda533ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribución de las otras variables categóricas\n",
    "columnas_category = ['color', 'clarity']\n",
    "for col in columnas_category:\n",
    "    print(data[col].value_counts())"
   ],
   "id": "4da848fbedd7a6c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Para mejor entendimiento de la columna <cut>, se realiza un gráfico de barras\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.countplot(x='cut', data=data, color=\"chocolate\", edgecolor=\"black\")\n",
    "plt.title('Distribución de la variable Cut', fontsize=16, fontweight=\"bold\", color=\"peru\")\n",
    "plt.show()\n"
   ],
   "id": "52bea6d828db0c59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*_LIMPIEZA DE DATOS_*\n",
    "\n",
    "1. Revisar valores nulos.\n",
    "2. Detectar y tratar valores atípicos (outliers).\n",
    "3. Revisar y convertir tipos de datos si es necesario.\n",
    "4. Eliminar columnas irrelevantes.\n",
    "\n",
    "Dado que en la anterior actividad, no llegué a tratar valores nulos ni duplicados, he decidido en esta, crear algunos y corregirlos posteriormente para practicar este paso (en la vida real, las bases de datos están llenas de estos)."
   ],
   "id": "9134ebadd4ae21e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# VALORES NULOS\n",
    "# Introducir valores nulos en algunas filas:\n",
    "# Cada 50, se añadirá un valor nulo en la columna 'carat'\n",
    "# Cada 75, se añadirá un valor nulo en la columna a predecir 'cut'\n",
    "# Cada 100, se añadirá un valor nulo en la columna 'price'\n",
    "\n",
    "data.loc[::50, 'carat'] = np.nan\n",
    "data.loc[::75, 'cut'] = np.nan\n",
    "data.loc[::100, 'price'] = np.nan"
   ],
   "id": "e440a0ba9d084ce9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Revisar valores nulos\n",
    "# Como se añadieron antes valores nulos, ahora SI deben aparecer aquí, diferenciándose de cuando se hizo más arriba\n",
    "data.isnull().sum()"
   ],
   "id": "ab9baea57b01ab39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Rellenar valores nulos con distintos procedimientos vistos en clase:\n",
    "# 'carat' -> la media\n",
    "# 'cut' -> la moda\n",
    "# 'price' -> la mediana\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Se crea un imputador para cada columna con la estrategia correspondiente\n",
    "imputer_carat = SimpleImputer(strategy='mean')\n",
    "imputer_cut = SimpleImputer(strategy='most_frequent')\n",
    "imputer_price = SimpleImputer(strategy='median')\n",
    "\n",
    "# Se aplica el imputador a cada columna\n",
    "data['carat'] = imputer_carat.fit_transform(data[['carat']]).flatten()\n",
    "data['cut'] = imputer_cut.fit_transform(data[['cut']]).flatten()\n",
    "data['price'] = imputer_price.fit_transform(data[['price']]).flatten()\n"
   ],
   "id": "c68dd852ca691e0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Revisamos de nuevo los valores nulos\n",
    "data.isnull().sum()"
   ],
   "id": "8a596a607fd1c21f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# OUTLIERS\n",
    "# Detectar y tratar valores atípicos:\n",
    "# Se puede detectar estos usando diagramas de caja con las columnas numéricas\n",
    "columnas_numerical = ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.set_style(\"white\")\n",
    "sns.boxplot(data=data[columnas_numerical], palette=\"viridis\")\n",
    "plt.title(\"Detección de valores atípicos\", fontsize=16, fontweight='bold', color=\"lightblue\")\n",
    "plt.show()"
   ],
   "id": "8cbbba433f5f7196",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Para tratar los outliers, se usará el valor inter cuartil, IQR\n",
    "def detectar_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] < limite_inferior) | (df[column] > limite_superior)]"
   ],
   "id": "d98792ee1d7466e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for columna in columnas_numerical:\n",
    "    outliers = detectar_outliers(data, columna)\n",
    "    print(f\"Outliers detectados en {columna}: {len(outliers)}\")\n",
    "    # Eliminar los outliers del dataset\n",
    "    data = data[~data.index.isin(outliers.index)]\n"
   ],
   "id": "1692be4e241d100e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualizar con un boxplot para confirmar que no hay valores atípicos\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.set_style(\"white\")\n",
    "sns.boxplot(data=data[columnas_numerical], palette=\"viridis\")\n",
    "plt.title('Boxplot de la columna depth después de eliminar outliers', fontsize=16, fontweight='bold', color=\"indianred\")\n",
    "plt.show()\n",
    "\n",
    "for columna in columnas_numerical:\n",
    "   outliers = detectar_outliers(data, columna)\n",
    "   print(f\"Outliers detectados en {columna}: {len(outliers)}\")"
   ],
   "id": "7e23e40bfdaaae7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vemos que, aunque se han reducido en gran medida los valores atípicos, aún no se han eliminado por completo. Esto puede ser normal dependiendo del contexto y el tipo de datos. Por ejemplo, los outliers pueden representar diamantes muy caros o muy grandes. Por ahora, he decidido no eliminarlos porque podría distorsionar el análisis.\n",
    "\n",
    "Más adelante, cuando se realize la preparación de datos, se probará el método de RobustScaler. Esta es una técnica de escalado que ayuda a reducir el impacto de los outliers en los datos. A diferencia de otros escaladores como StandardScaler (que usa la media y la desviación estándar), RobustScaler usa la mediana y el rango intercuartílico (IQR), lo que lo hace menos sensible a valores extremos. Como en la anterior actividad utilizé el StandardScaler, en esta probaré esta técnica para ver su funcionamiento y entender su diferencia."
   ],
   "id": "7598bc14d90c3a7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TIPOS DE DATOS:\n",
    "# Revisar y convertir tipos de datos si es necesario\n",
    "data.dtypes"
   ],
   "id": "457002e3e35a2341",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Voy a corregir las columnas 'cut', 'color', 'clarity' cambiándolas a tipo categoría\n",
    "data = data.assign(\n",
    "    cut=data['cut'].astype('category'),\n",
    "    color=data['color'].astype('category'),\n",
    "    clarity=data['clarity'].astype('category')\n",
    ")\n",
    "data.dtypes # Comprobación del arreglo"
   ],
   "id": "c869cc23069d604e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# COLUMNAS IRRELEVANTES:\n",
    "# Eliminar columnas irrelevantes\n",
    "# En este dataset, la columna 'x', 'y', 'z'  se pueden considerar redundantes\n",
    "data.drop(columns=['x', 'y', 'z'], inplace=True)"
   ],
   "id": "c91a3352fba625b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*_ANÁLISIS EXPLORATORIO DE DATOS (EDA)_*\n",
    "\n",
    "- EDA Univariante: Objetivo -> Entender la distribución de cada variable por separado.\n",
    "- EDA Bivariante: Objetivo -> Explorar la relación entre dos variables.\n",
    "- EDA Multivariante: Objetivo -> Entender la interacción entre más de dos variables.\n"
   ],
   "id": "473b307a9c5c3bc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# EDA UNIVARIANTE:\n",
    "# Se realizan histograma de las columnas numéricas\n",
    "variables_numeric = ['carat', 'depth', 'table', 'price']\n",
    "\n",
    "for col in variables_numeric:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.set_style(\"white\")\n",
    "    sns.histplot(data[col], kde=True, bins=30, color=\"olivedrab\", edgecolor=\"black\")\n",
    "    plt.title(f'Distribución de {col}', fontsize=16, fontweight=\"bold\", color=\"y\")\n",
    "    plt.xlabel(col, fontsize=16, fontweight=\"bold\", color=\"darkseagreen\")\n",
    "    plt.ylabel('Frecuencia', fontsize=16, fontweight=\"bold\", color=\"darkseagreen\")\n",
    "    plt.show()"
   ],
   "id": "76df060fdc82ea96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Estadísticas descriptivas para variables numéricas\n",
    "data[variables_numeric].describe()"
   ],
   "id": "985e649f7527c35c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Análisis de variables categóricas\n",
    "# Se realizan histogramas de las variables categóricas\n",
    "variables_categoric = ['cut', 'color', 'clarity']\n",
    "\n",
    "for col in variables_categoric:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(x=data[col], order=data[col].value_counts().index, color=\"firebrick\", edgecolor=\"black\")\n",
    "    plt.title(f'Frecuencia de {col}', fontsize=16, fontweight=\"bold\", color=\"sienna\")\n",
    "    plt.xlabel(col, fontsize=16, fontweight=\"bold\", color=\"indianred\")\n",
    "    plt.ylabel('Frecuencia', fontsize=16, fontweight=\"bold\", color=\"indianred\")\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "3b9869ca4afc5c02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Frecuencia de las variables categóricas\n",
    "for col in variables_categoric:\n",
    "    print(f\"\\nDistribución de {col}:\")\n",
    "    print(data[col].value_counts())"
   ],
   "id": "e5d8e123ba8a964c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "En estas semanas también hemos visto la simetría o la asimetría de una distribución. Esta, medida con .skew(), forma parte del EDA univariante. Se utiliza principalmente para entender la distribución de las variables numéricas y evaluar si están sesgadas hacia la derecha (asimetría positiva) o hacia la izquierda (asimetría negativa).",
   "id": "78caa132c96f72d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Simetría de las variables numéricas\n",
    "data[variables_numeric].skew()\n"
   ],
   "id": "1865f8f796e535e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Las variables carat, table, y price presentan asimetría a la derecha (sesgo positivo), lo que significa que la mayoría de los datos están concentrados en el lado inferior de su rango. Algunos valores son más altos y arrastran la cola de la distribución hacia la derecha.\n",
    "En cambio, Depth tiene una ligera asimetría a la izquierda (sesgo negativo), lo que indica que la mayoría de los valores son relativamente altos, pero con algunos valores más bajos."
   ],
   "id": "79d8f50b9dadc470"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#EDA BIVARIANTE:\n",
    "# Diagrama de dispersión entre carat y price\n",
    "sns.set_style(\"white\")\n",
    "sns.scatterplot(data=data, x='carat', y='price', color=\"palevioletred\", edgecolor=\"black\")\n",
    "plt.title('Relación entre Carat y Price', fontsize=16, fontweight=\"bold\", color=\"violet\")\n",
    "plt.xlabel('Carat', fontsize=16, fontweight=\"bold\", color=\"rosybrown\")\n",
    "plt.ylabel('Price', fontsize=16, fontweight=\"bold\", color=\"rosybrown\")\n",
    "plt.show()"
   ],
   "id": "da2fd2d37a990ba0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Diagrama de dispersión entre depth y price\n",
    "sns.set_style(\"white\")\n",
    "sns.scatterplot(data=data, x='depth', y='price', color=\"goldenrod\", edgecolor=\"black\" )\n",
    "plt.title('Relación entre Depth y Price', fontsize=16, fontweight=\"bold\", color=\"tan\")\n",
    "plt.xlabel('Depth', fontsize=16, fontweight=\"bold\", color=\"moccasin\")\n",
    "plt.ylabel('Price', fontsize=16, fontweight=\"bold\", color=\"moccasin\")\n",
    "plt.show()"
   ],
   "id": "ae875a66a535b1ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "En el primer caso, vemos como Price y Carat tienen una relación muy alta, a mayor tamaño, mayor será el precio. Sin embargo, no podemos decir lo mismo en la otra comparación con la profundidad.",
   "id": "c530d4cefd33002e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Boxplot de price según cut\n",
    "sns.set_style(\"white\")\n",
    "sns.boxplot(data=data, x='cut', y='price', color=\"lightcoral\")\n",
    "plt.title('Distribución de Price por Cut',  fontsize=16, fontweight=\"bold\", color=\"red\")\n",
    "plt.xlabel('Cut', fontsize=16, fontweight=\"bold\", color=\"darkred\")\n",
    "plt.ylabel('Price', fontsize=16, fontweight=\"bold\", color=\"darkred\")\n",
    "plt.show()"
   ],
   "id": "96e16732972de4ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Boxplot de carat según cut\n",
    "sns.set_style(\"white\")\n",
    "sns.boxplot(data=data, x='cut', y='carat',color=\"lightcyan\")\n",
    "plt.title('Distribución de Carat por Cut',  fontsize=16, fontweight=\"bold\", color=\"turquoise\")\n",
    "plt.xlabel('Cut', fontsize=16, fontweight=\"bold\", color=\"skyblue\")\n",
    "plt.ylabel('Carat', fontsize=16, fontweight=\"bold\", color=\"skyblue\")\n",
    "plt.show()"
   ],
   "id": "f1f8ed981965bbc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Los boxplots muestran las diferencias en la mediana, los cuartiles y los valores atípicos entre las categorías de cut. Por ejemplo, se puede ver que el cut más alto tiende a tener diamantes de mayor precio y mayor tamaño.",
   "id": "f1ac8fc41da95987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Crear una tabla de contingencia entre dos variables categóricas\n",
    "contingency_table = data.pivot_table(index=['cut', 'color'], columns='clarity', aggfunc='size', fill_value=0, observed=False)\n",
    "contingency_table\n"
   ],
   "id": "38eb05784aef787d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una tabla de contingencia es una forma de visualizar cómo se distribuyen las categorías de dos o más variables categóricas. Te permite ver cuántas veces aparecen combinaciones específicas de categorías.",
   "id": "4a76b5a085698002"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gráfico de barras apiladas comparando 'cut' y 'color'\n",
    "sns.set_style('whitegrid')\n",
    "pd.crosstab(data['cut'], data['color']).plot(kind='bar', stacked=True, cmap='viridis')\n",
    "plt.title('Distribución de Cut y Color', fontsize=16, fontweight=\"bold\", color=\"mediumspringgreen\")\n",
    "plt.xlabel('Cut', fontsize=16, fontweight=\"bold\", color=\"teal\")\n",
    "plt.ylabel('Frecuencia', fontsize=16, fontweight=\"bold\", color=\"teal\")\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de barras apiladas comparando 'cut' y 'clarity'\n",
    "sns.set_style('whitegrid')\n",
    "pd.crosstab(data['cut'], data['clarity']).plot(kind='bar', stacked=True, cmap='flare')\n",
    "plt.title('Distribución de Cut y Clarity', fontsize=16, fontweight=\"bold\", color=\"plum\")\n",
    "plt.xlabel('Cut', fontsize=16, fontweight=\"bold\", color=\"palevioletred\")\n",
    "plt.ylabel('Frecuencia', fontsize=16, fontweight=\"bold\", color=\"palevioletred\")\n",
    "plt.show()\n"
   ],
   "id": "724109702cdfa66a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Un gráfico de barras apiladas es útil para ver la distribución de las categorías en relación con otras variables categóricas. Esto te permite comparar cómo se distribuyen las categorías dentro de otra categoría.",
   "id": "5ba825db844ec72c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculando la matriz de correlación entre las variables numéricas\n",
    "corr_matrix = data[['carat', 'depth', 'table', 'price']].corr()\n",
    "\n",
    "# Mapa de calor de la matriz de correlación\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_style(\"white\")\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='rocket', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Mapa de Calor de Correlación entre Variables Numéricas', fontsize=16, fontweight=\"bold\", color=\"red\")\n",
    "plt.show()\n"
   ],
   "id": "39d403cb63aa4867",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Se puede ver la relación de las variables categóricas utilizando la matriz de correlación transformándolas en variables numéricas mediante técnicas Label Encoding.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Codificar variables categóricas en valores numéricos\n",
    "le = LabelEncoder()\n",
    "data_codify = data.copy()\n",
    "data_codify['cut'] = le.fit_transform(data['cut'])\n",
    "data_codify['color'] = le.fit_transform(data['color'])\n",
    "data_codify['clarity'] = le.fit_transform(data['clarity'])\n",
    "\n",
    "# Matriz de correlación\n",
    "corr_matrix_categor = data_codify[['cut', 'color', 'clarity']].corr()\n",
    "\n",
    "# Mapa de calor de la correlación\n",
    "sns.set_style('whitegrid')\n",
    "sns.heatmap(corr_matrix_categor, annot=True, cmap='viridis_r', fmt='.2f')\n",
    "plt.title('Mapa de Calor de Correlación entre Variables Categóricas', fontsize=16, fontweight=\"bold\", color=\"deepskyblue\")\n",
    "plt.show()\n"
   ],
   "id": "14acc65e1f1c2bce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#EDA MULTIVARIANTE:\n",
    "# Pairplot de las variables numéricas (carat, depth, table, price)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_style(\"white\")\n",
    "sns.pairplot(data[['carat', 'depth', 'table', 'price']], diag_kind='kde')\n",
    "plt.suptitle('Gráfico de Pares: Relación entre variables', y=1.02, fontsize=16, fontweight=\"bold\", color=\"royalblue\")\n",
    "plt.show()\n"
   ],
   "id": "8632c41606ae1330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Un gráfico de pares o pairplot muestra todas las combinaciones de pares de variables, lo que te permite ver patrones, correlaciones y posibles agrupamientos entre ellas.",
   "id": "644690b1e115afab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Crear un gráfico de violín para cada columna numérica en función de 'cut'\n",
    "col_numeric = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in col_numeric:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.violinplot(data=data, hue='cut', y=col, palette='magma')\n",
    "    plt.title(f'Distribución de {col} por Cut', fontsize=16, fontweight=\"bold\", color=\"tomato\")\n",
    "    plt.legend(title='Cut', loc='center', bbox_to_anchor=(0.5, -0.05), ncol=2)\n",
    "    plt.show()"
   ],
   "id": "4b2252f9e4efa68b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Los gráficos de violín son útiles para comparar las distribuciones de una variable numérica en diferentes categorías. En nuestro caso, se puede comparar la distribución de variables como price, carat, etc., para cada categoría de cut. Los violines más anchos indican una mayor concentración de valores en esa área. Es útil para ver si las distribuciones de price o carat son similares o difieren dependiendo del cut.",
   "id": "dc73e82be2a63213"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Como en las clases también hemos mencionado la libreria plotly express para realizar gráficos interactivos, voy a trabajar un poco con ella.",
   "id": "2c7b4932176ba129"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#Gráfico de dispersión 3D para visualizar la relación entre tres variables numéricas: carat, depth y price. Además, usaremos el parámetro color para diferenciar los puntos según la variable categórica cut.\n",
    "\n",
    "fig = px.scatter_3d(data,\n",
    "                    x='carat',\n",
    "                    y='depth',\n",
    "                    z='price',\n",
    "                    color='cut',\n",
    "                    title=\"Gráfico 3D: Relación entre Carat, Depth y Price\",\n",
    "                    labels={\"carat\": \"Carat\", \"depth\": \"Depth\", \"price\": \"Price\"})\n",
    "\n",
    "\n",
    "fig.show()\n"
   ],
   "id": "1b8627a7d15a2f33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.line(data, x=\"price\", y=\"carat\", color=\"cut\", line_group=\"cut\", title=\"Carat vs Price por Cut\")\n",
    "fig.show()\n"
   ],
   "id": "6b5beb10c4d5888b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Seleccionamos las variables numéricas\n",
    "df_num = data[['carat', 'depth', 'table', 'price']]\n",
    "\n",
    "# Estandarizamos las variables\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_num)\n",
    "\n",
    "# Aplicamos PCA\n",
    "pca = PCA(n_components=2)  # Reduciendo a 2 componentes principales\n",
    "pca_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Creamos un DataFrame con los componentes principales\n",
    "pca_df = pd.DataFrame(pca_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Gráfica con los componentes principales\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.5, cmap='flare', c=data['price'])\n",
    "plt.title('PCA: Componentes Principales', fontsize=16, fontweight=\"bold\", color=\"red\")\n",
    "plt.xlabel('Componente Principal 1', fontsize=16, fontweight=\"bold\", color=\"tomato\")\n",
    "plt.ylabel('Componente Principal 2', fontsize=16, fontweight=\"bold\", color=\"tomato\")\n",
    "plt.colorbar(label='Price')\n",
    "plt.show()\n",
    "\n",
    "# Ver la varianza explicada por los componentes\n",
    "print(f\"Varianza explicada por PC1: {pca.explained_variance_ratio_[0]:.4f}\")\n",
    "print(f\"Varianza explicada por PC2: {pca.explained_variance_ratio_[1]:.4f}\")\n",
    "print(f\"Varianza total explicada: {sum(pca.explained_variance_ratio_):.4f}\")\n"
   ],
   "id": "ac369ec7170662d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "PCA es útil cuando se tienen muchas variables numéricas y se desea reducir la dimensionalidad mientras se mantiene la mayor parte de la variabilidad en los datos. Esto nos puede ayudar a visualizarlos de forma más clara y también identificar patrones o agrupamientos. En el gráfico de PCA, se puede ver cómo los datos se agrupan en función de los componentes principales.\n",
    "\n",
    "En este caso, vemos que los datos están agrupados en el centro, lo que puede sugerir una baja Varianza en los datos originales, los datos están muy correlacionados, o incluso la estandarización es insuficiente o incorrecta. Por este último motivo vamos a aplicar el método mencionado anteriormente, RobustScaler.\n"
   ],
   "id": "36b249c38cbb3b38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Estandarizamos las variables con RobustScaler\n",
    "scaler = RobustScaler()\n",
    "df_robust_scaled = scaler.fit_transform(df_num)\n",
    "\n",
    "# Aplicamos PCA\n",
    "pca = PCA(n_components=2)  # Reduciendo a 2 componentes principales\n",
    "pca_components = pca.fit_transform(df_robust_scaled)\n",
    "\n",
    "# Crear un DataFrame con los componentes principales\n",
    "pca_robust_df = pd.DataFrame(pca_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Graficar los componentes principales\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_robust_df['PC1'], pca_robust_df['PC2'], alpha=0.5, cmap='viridis', c=data['price'])\n",
    "plt.title('PCA con RobustScaler: Componentes Principales',fontsize=16, fontweight=\"bold\", color=\"c\")\n",
    "plt.xlabel('Componente Principal 1', fontsize=16, fontweight=\"bold\", color=\"dodgerblue\")\n",
    "plt.ylabel('Componente Principal 2', fontsize=16, fontweight=\"bold\", color=\"dodgerblue\")\n",
    "plt.colorbar(label='Price')\n",
    "plt.show()\n",
    "\n",
    "# Ver la varianza explicada por los componentes\n",
    "print(f\"Varianza explicada por PC1: {pca.explained_variance_ratio_[0]:.4f}\")\n",
    "print(f\"Varianza explicada por PC2: {pca.explained_variance_ratio_[1]:.4f}\")\n",
    "print(f\"Varianza total explicada: {sum(pca.explained_variance_ratio_):.4f}\")\n"
   ],
   "id": "f153f5f5d3695617",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Resultados:\n",
    "La varianza total explicada es muy similar en ambos casos, lo que significa que, en términos generales, ambas escalas logran capturar prácticamente la misma cantidad de información sobre los datos. StandardScaler parece explicar mejor la varianza en el primer componente principal (PC1), con un 49.35% frente al 41.96% de RobustScaler. Esto indica que, con StandardScaler, el primer componente principal captura más de la variabilidad de los datos. Sin embargo, RobustScaler parece distribuir la varianza de manera más equilibrada entre PC1 (41.96%) y PC2 (37.45%).\n",
    "\n",
    "Dado que nuestros datos tienen outliers o valores extremos, RobustScaler es una la mejor opción, ya que se basa en la mediana y el rango intercuartílico, haciéndola menos sensible a los outliers."
   ],
   "id": "52b20c0e95e079aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*_PREPARACIÓN DE DATOS PARA EL MODELADO_*\n",
    "\n",
    "Resumen del proceso de preparación de datos:\n",
    "\n",
    "+ Manejo de valores nulos → Imputación o eliminación. ✓\n",
    "+ Codificación de variables categóricas → One-Hot o Label Encoding. ✓\n",
    "+ Escalado de datos numéricos → Estandarización o normalización.\n",
    "+ Eliminación de variables irrelevantes ✓\n",
    "+ División en conjuntos de entrenamiento y prueba.\n",
    "\n",
    "✓ Indica aquellos pasos ya realizados.\n",
    "\n"
   ],
   "id": "8284f813c8ece37f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Revisamos la codificación con Label Encoding\n",
    "data_codify.head()"
   ],
   "id": "1fe7a959f429bc9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se añade nuevas columnas que aporten mayor información al dataset\n",
    "# Precio por quilate: Puede ayudar a diferenciar calidades\n",
    "data_codify['price_per_carat'] = data_codify['price'] / data_codify['carat']\n",
    "\n",
    "# Relación depth con table: Puede indicar qué proporción del diamante está en la tabla, lo que influye en su brillo\n",
    "data_codify['depth_table_ratio'] = data_codify['depth'] / data_codify['table']\n",
    "\n",
    "# Relación entre color y clarity: Algunas combinaciones de color y claridad pueden influir en el cut.\n",
    "data_codify['color_clarity'] = data_codify['color'].astype(str) + \"_\" + data_codify['clarity'].astype(str)\n",
    "\n",
    "data_codify.head()"
   ],
   "id": "bb1d57fb78c007ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se aplicar Label Encoding con la columna color_clarity_interaction\n",
    "encoder = LabelEncoder()\n",
    "data_codify['color_clarity'] = encoder.fit_transform(data_codify['color_clarity'])"
   ],
   "id": "91a68bd4a77082b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Se separa la variable (y) y las variables predictoras (X)\n",
    "X = data_codify.drop(columns=['cut'])\n",
    "y = data_codify['cut']\n",
    "\n",
    "# Se aplica RobustScaler solo a las variables numéricas\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Se convierte a DataFrame para facilitar su uso\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Se imprime para comprobarlo\n",
    "X_scaled.head()"
   ],
   "id": "5b927a328d63fd9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se divide los datos en entrenamiento y prueba (80% train - 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Se verifica el tamaño de ambos\n",
    "print(f\"Tamaño de X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"Tamaño de y_train: {y_train.shape}, y_test: {y_test.shape}\")\n"
   ],
   "id": "b4ee16f22647f25d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*_MODELADO_*\n",
    "\n",
    "En mi caso, he decidido usar regresión Logística (Baseline simple), Random Forest (Modelo basado en árboles), XGBoost (Modelo avanzado de boosting) y una red neuronal con TensorFlow. Al final se comparará el rendimiento de todos y se tomará una decision sobre cuál será mejor emplear.\n",
    "\n",
    "Pasos para el modelado:\n",
    "\n",
    "- Importar librerías necesarias.\n",
    "- Optimización de hiperparámetros\n",
    "- Crear y entrenar el modelo de regresión lineal.\n",
    "- Realizar predicciones con el conjunto de prueba.\n",
    "- Evaluar el modelo usando métricas adecuadas.\n"
   ],
   "id": "d103e0eab5b70ce1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Modelos con Scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Lista de modelos con sus respectivos nombres\n",
    "modelos = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=42),\n",
    "    \"SVC\": SVC(random_state=42),\n",
    "    \"K-Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Diccionario para almacenar las métricas\n",
    "resultados_sklearn = {}\n",
    "\n",
    "# Entrenamiento y evaluación de los modelos mencionados antes\n",
    "for nombre, modelo in modelos.items():\n",
    "    modelo.fit(X_train, y_train)  # Se entrena el modelo\n",
    "    y_pred = modelo.predict(X_test)  # Se predicen los resultados\n",
    "    acc = accuracy_score(y_test, y_pred)  # Se calcula la precisión\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')  # Promedio ponderado\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    # Validación cruzada con 5 folds\n",
    "    cross_val_acc = cross_val_score(modelo, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    # Se guardan los resultados\n",
    "    resultados_sklearn[nombre] = {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Cross-Val Accuracy\": cross_val_acc\n",
    "    }\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n Comparación de Modelos Scikit-Learn:\")\n",
    "for nombre, metricas in resultados_sklearn.items():\n",
    "    print(f\"\\n Modelo --> {nombre}:\")\n",
    "    for metrica, valor in metricas.items():\n",
    "        print(f\" Metrica --> {metrica}: Valor --> {valor:.4f}\")\n"
   ],
   "id": "6f86fc04344a0041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "_*CONCLUSIONES*_\n",
    "\n",
    "1. La regresión logística muestra una precisión moderada, pero su F1-Score es relativamente bajo, lo que sugiere un desequilibrio entre las métricas de precisión y recall. El modelo tiene un desempeño aceptable, pero no es el mejor en términos de precisión y F1-Score.\n",
    "\n",
    "2. El Random Forest muestra un buen rendimiento en todas las métricas, especialmente en términos de precisión y recall, con una F1-Score decente. Este modelo parece ser una de las opciones más equilibradas y tiene un buen desempeño en validación cruzada. Es un modelo robusto para clasificación.\n",
    "\n",
    "3. El XGBoost se destaca como el mejor modelo en todas las métricas clave, superando a los demás en precisión, recall y F1-Score. También tiene un excelente rendimiento en validación cruzada. Es el modelo más efectivo y parece ser la mejor opción si se busca maximizar el rendimiento.\n",
    "\n",
    "4. SVC (Support Vector Classifier) presenta un rendimiento sólido, con un equilibrio entre precisión y recall. Aunque no supera a XGBoost o Random Forest, sigue siendo una opción competitiva con una buena precisión y rendimiento general.\n",
    "\n",
    "5. K-Neighbors tiene un rendimiento moderado, similar al de la regresión logística, aunque con una precisión algo mayor. Sin embargo, su F1-Score sigue siendo inferior al de Random Forest y XGBoost, lo que sugiere que no es tan efectivo en términos de balance entre precisión y recall.\n",
    "\n",
    "En resumen: XGBoost posee mejor rendimiento general con la mayor precisión, recall, F1-Score y cross-validation. Le acompaña Random Forest, con un buen rendimiento aunque ligeramente inferior y posee un desempeño consistente en todas las métricas. El SVC tiene un rendimiento competitivo, pero no tan fuerte como los dos anteriores, ofreciendo un buen equilibrio en precisión y recall. Finalmente, Logistic Regression y K-Neighbors tienen resultados relativamente cercanos, pero no alcanzan el rendimiento de los modelos más avanzados como XGBoost y Random Forest.\n",
    "\n",
    "Si se busca el mejor desempeño posible, XGBoost es la opción más adecuada. Por otra parte, Random Forest es una excelente alternativa si se busca un modelo robusto y equilibrado. También, SVC puede ser útil si se necesita un modelo que balancee precisión y recall sin requerir demasiado ajuste. Por último, Logistic Regression y K-Neighbors son modelos más simples y adecuados si el objetivo es obtener resultados rápidos o si los recursos son limitados.\n"
   ],
   "id": "f1b98573ff5f025c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
